from torch.nn import (
    Threshold,
    ReLU,
    Hardtanh,
    ReLU6,
    Sigmoid,
    Tanh,
    Softmax,
    Softmax2d,
    LogSoftmax,
    ELU,
    SELU,
    CELU, GELU, Hardshrink, LeakyReLU, LogSigmoid,
    Softplus,
    Softshrink,
    MultiheadAttention,
    PReLU,
    Softsign,
    Softmin, Tanhshrink, RReLU, GLU,
    Hardsigmoid,
    Hardswish,
    SiLU,
    Mish
)

available_activations = {
    "Threshold": Threshold,
    "ReLU": ReLU,
    "Hardtanh": Hardtanh,
    "ReLU6": ReLU6,
    "Sigmoid": Sigmoid,
    "Tanh": Tanh,
    "Softmax": Softmax,
    "Softmax2d": Softmax2d,
    "LogSoftmax": LogSoftmax,
    "ELU": ELU,
    "SELU": SELU,
    "CELU": CELU,
    "GELU": GELU,
    "Hardshrink": Hardshrink,
    "LeakyReLU": LeakyReLU,
    "LogSigmoid": LogSigmoid,
    "Softplus": Softplus,
    "Softshrink": Softshrink,
    "MultiheadAttention": MultiheadAttention,
    "PReLU": PReLU,
    "Softsign": Softsign,
    "Softmin": Softmin,
    "Tanhshrink": Tanhshrink,
    "RReLU": RReLU,
    "GLU": GLU,
    "Hardsigmoid": Hardsigmoid,
    "Hardswish": Hardswish,
    "SiLU": SiLU,
    "Mish": Mish
}
